{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Trace clustering and Context Attribute ranking.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPIEJCkdx+KiZx2cfS8Of3b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ejakupi13/Uni-Projects/blob/main/Trace_clustering_and_Context_Attribute_ranking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OFqnJxcRk6V"
      },
      "source": [
        "pip install pm4py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqWYAl4ERTZ7",
        "outputId": "23c55515-df38-45b0-9f2d-e4319b2c48df"
      },
      "source": [
        "import pm4py\n",
        "print(pm4py.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_aK0wATR2fd"
      },
      "source": [
        "#Mount the data from google drive, need to specify the path\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls \"/content/drive/My Drive/\"\n",
        "%cd /content/drive/My Drive/Colab Notebooks/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDobgbDORHwe"
      },
      "source": [
        "\n",
        "\n",
        "# Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItcntZPjSIOp"
      },
      "source": [
        "import pandas as pd\n",
        "from pm4py.objects.log.util import dataframe_utils\n",
        "from pm4py.objects.conversion.log import converter as log_converter\n",
        "\n",
        "#read the event logs from csv files \n",
        "log = pd.read_csv('file.csv', sep=',')\n",
        "\n",
        "\"\"\"  read the event logs from xes files and convert to dataframe since the rest of the code works with dataframes\n",
        "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
        "xes = xes_importer.apply('file.xes')\n",
        "log = log_converter.apply(xes, variant=log_converter.Variants.TO_DATA_FRAME) \"\"\"\n",
        "\n",
        "#convert timestamp to the desired format\n",
        "log = dataframe_utils.convert_timestamp_columns_in_df(log)\n",
        "log['time:timestamp'] = pd.to_datetime(log['time:timestamp'], format = \"%d-%m-%Y %H:%M:%S.%f\" )\n",
        "\n",
        "#sort dataframe by timestamp\n",
        "log = log.sort_values('time:timestamp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OGt5GVrT4ud"
      },
      "source": [
        "Preprocess event logs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxaNcHfHT3Ii"
      },
      "source": [
        "#remove duplicate entries\n",
        "def duplicate_removal(log):\n",
        "    return log.drop_duplicates(keep='first')\n",
        "\n",
        "#filter out traces with less than 3 events\n",
        "def filter_short_traces(log):\n",
        "    return log.groupby('case concept:name').filter(lambda x: x['concept:name'].count() > 2 )\n",
        "\n",
        "#remove traces that do not have case, event or time identifier (mandatory attributes)\n",
        "def filter_mandatory(log):\n",
        "    return log.dropna(axis=0, subset=['case:concept:name','concept:name', 'time:timestamp'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3R_RogIttx9"
      },
      "source": [
        "# Feature Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh8KVIsih7Ki"
      },
      "source": [
        "Parts of this section's code is adapted from: https://github.com/p-decker/Event_Log_Assessment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfPswWywt-K_"
      },
      "source": [
        "Count number of events per each trace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55MJEc5AtzhU"
      },
      "source": [
        "def event_count(log):\n",
        "    return log.groupby('case:concept:name')['concept:name'].count().values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVBzJZd1uLNQ"
      },
      "source": [
        "Count number of distinct events per trace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRu0JJ95uTXk"
      },
      "source": [
        "def event_count_unique(log):\n",
        "    return log.groupby('case:concept:name')['concept:name'].nunique().values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cyz7DYaDzceT"
      },
      "source": [
        "Return a variant number for each trace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LckhgrLDzctv"
      },
      "source": [
        "#variant code\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "def variant(log):\n",
        "    variants = log.groupby('case:concept:name')['concept:name'].apply(tuple).reset_index(name ='variants')\n",
        "    variants[\"variant\"] = OrdinalEncoder().fit_transform(variants[[\"variants\"]])\n",
        "    return variants[\"variant\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRcWQeztuU07"
      },
      "source": [
        "Calculate size of self loops per trace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_xJBHV_u0ra"
      },
      "source": [
        "def self_loop_per_trace_overview(log):\n",
        "    self_loop_overview_list = []\n",
        "    trace_list = log.groupby('case:concept:name')['concept:name'].apply(list).values.tolist()\n",
        "    # create trace representation\n",
        "    for trace in trace_list:        \n",
        "        # create list containing size of loops in trace\n",
        "        self_loop_size_list = []\n",
        "        i = 0\n",
        "        while i < (len(trace) - 1):\n",
        "            self_loop_size = 0\n",
        "            if trace[i] == trace[i + 1]:\n",
        "                self_loop_size += 1\n",
        "                for k in range(i + 1, len(trace)-1, 1):\n",
        "                    #check consecutive events\n",
        "                    if trace[k] == trace[k+1]:\n",
        "                        self_loop_size += 1\n",
        "            if self_loop_size > 0:\n",
        "                self_loop_size_list.append(self_loop_size)\n",
        "                i += self_loop_size\n",
        "            else:\n",
        "                i += 1\n",
        "        self_loop_overview_list.append(sum(self_loop_size_list))\n",
        "    return self_loop_overview_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_Ed3HP2u3rp"
      },
      "source": [
        "Calculate number of repetitions per trace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOhNyoB9v9C3"
      },
      "source": [
        "def repetition_per_trace_overview(log):\n",
        "    repetition_overview_list = []\n",
        "    trace_list = log.groupby('case:concept:name')['concept:name'].apply(list).values.tolist()\n",
        "    # create trace representation\n",
        "    for trace in trace_list:\n",
        "        # create window to detect repetitions\n",
        "        window = []\n",
        "        repetition_size_list = []\n",
        "        repetition_size = 0\n",
        "        # append events to window\n",
        "        for i in trace:\n",
        "            if i not in window:\n",
        "                window.append(i)\n",
        "            else:\n",
        "                # check if repetition is not a self-loop\n",
        "                position = len(window) - 1 - window[::-1].index(i)\n",
        "                if position == (len(window) - 1):\n",
        "                    window.append(i)\n",
        "                else:\n",
        "                    # calculate repetition size and delete repetition from window\n",
        "                    repetition_size += len(window[position: (len(window) + 1)])\n",
        "                    repetition_size_list.append(repetition_size)\n",
        "                    del window[position: (len(window) + 1)]\n",
        "                    window.append(i)\n",
        "        repetition_overview_list.append(len(repetition_size_list))\n",
        "    return repetition_overview_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCCZ1tC5wBWl"
      },
      "source": [
        "Convert trace representation to scalar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cM-RAu5jwE0Q"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "def cat2num(log):\n",
        "    log_copy = log.copy()\n",
        "    log_copy[\"counter\"] = 1\n",
        "    log_copy = log_copy.groupby(['case:concept:name', 'concept:name'])['counter'].count().reset_index()\n",
        "    return log_copy.groupby('case:concept:name')['counter'].apply(lambda x: sum(x**x)).values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d2ayDLFwl5C"
      },
      "source": [
        "Methods to create a directed graph and calculate network connectivity for each trace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXKdUmmqwk_x"
      },
      "source": [
        "# Returns the names of events in the log (trace)\n",
        "def event_names(log):\n",
        "    from pm4py.algo.filtering.pandas.attributes import attributes_filter\n",
        "    events = attributes_filter.get_attribute_values(log, \"concept:name\")\n",
        "    event_list = [*events]\n",
        "    return event_list\n",
        "\n",
        "# Returns a dictionary that assigns a distinct number to each event in log (trace)\n",
        "def ranking_dict(log):\n",
        "    events = sorted(event_names(log))\n",
        "    rank_dict = {b: a for a, b in enumerate(events)}\n",
        "    return rank_dict\n",
        "\n",
        "# Creates an adjacency matrix for building a directed graph\n",
        "def adjacency_matrix_directed(log):\n",
        "    from pm4py.objects.dfg.retrieval.pandas import  get_dfg_graph\n",
        "    import numpy as np\n",
        "\n",
        "    event_ranking = ranking_dict(log)\n",
        "    connections = [list(i) for i in [*(get_dfg_graph(log,  measure=\"frequency\", sort_caseid_required=False, sort_timestamp_along_case_id=True, keep_once_per_case=False, window=1))]]\n",
        "\n",
        "    # Bring connections in rank format\n",
        "    for connection in range(len(connections)):\n",
        "        for elem in range(len(connections[connection])):\n",
        "            connections[connection][elem] = event_ranking[connections[connection][elem]]\n",
        "\n",
        "    # Create initial matrix\n",
        "    matrix_shape = len(event_names(log))\n",
        "    adjac_matrix = np.zeros(shape=(matrix_shape, matrix_shape))\n",
        "\n",
        "    # Fill matrix based on connections\n",
        "    for (i, j) in connections:\n",
        "        adjac_matrix[i][j] += 1\n",
        "    return np.asarray(adjac_matrix)\n",
        "\n",
        "#graph creation\n",
        "def create_directed_graph(log):\n",
        "    import networkx as nx\n",
        "\n",
        "    DG = nx.DiGraph()\n",
        "    matrix = adjacency_matrix_directed(log)\n",
        "    number_for_event = ranking_dict(log)\n",
        "    event_for_number = dict((y, x) for x, y in number_for_event.items())\n",
        "    for row in range(len(matrix)):\n",
        "        for event in range(len(matrix[row])):\n",
        "                if matrix[row][event] > 0:\n",
        "                    DG.add_edge(event_for_number[row], event_for_number[event])\n",
        "    return DG\n",
        "\n",
        "def number_of_arcs(log):\n",
        "   # from graph_creation import create_directed_graph\n",
        "    import networkx as nx\n",
        "    graph = create_directed_graph(log)\n",
        "    return len(nx.edges(graph))\n",
        "\n",
        "def number_of_nodes(log):\n",
        "   # from graph_creation import create_directed_graph\n",
        "    import networkx as nx\n",
        "    graph = create_directed_graph(log)\n",
        "    return len(nx.nodes(graph))\n",
        "    \n",
        "def network_connectivity(log):\n",
        "    from pandas import DataFrame\n",
        "    network_connectivity= []\n",
        "    trace_list = list(log.groupby(\"case:concept:name\", as_index=False))\n",
        "    for i in range(len(trace_list)):\n",
        "        trace = DataFrame(trace_list[i][1])\n",
        "        coeff = number_of_arcs(trace) / number_of_nodes(trace) \n",
        "        network_connectivity.append(coeff)\n",
        "    return network_connectivity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgV5g1wRXyVe"
      },
      "source": [
        "Create in degree matrix and convert to scalar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZqVEzsI4H-k"
      },
      "source": [
        "def in_degree(log):\n",
        "    from pandas import DataFrame\n",
        "    import math\n",
        "    trace_list = list(log.groupby(\"case:concept:name\", as_index=False))\n",
        "    in_degree = []\n",
        "    for i in range(len(trace_list)):\n",
        "        trace = DataFrame(trace_list[i][1])\n",
        "        graph = create_directed_graph(trace)\n",
        "        node_indegree_tuple_list=list(graph.in_degree()) \n",
        "        in_degree_list=[x[1] for x in node_indegree_tuple_list]\n",
        "        trace_indegree = math.sqrt(sum(map(lambda x:x*x,in_degree_list)))\n",
        "        in_degree.append(trace_indegree)\n",
        "    return in_degree\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkYLDXstYEnM"
      },
      "source": [
        "Create out degree matrix and convert to **scalar**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kINJqG4-E2lP"
      },
      "source": [
        "def out_degree(log):\n",
        "    from pandas import DataFrame\n",
        "    import math\n",
        "    trace_list = list(log.groupby(\"case:concept:name\", as_index=False))\n",
        "    out_degree = []\n",
        "    for i in range(len(trace_list)):\n",
        "        trace = DataFrame(trace_list[i][1])\n",
        "        graph = create_directed_graph(trace)\n",
        "        node_outdegree_tuple_list=list(graph.out_degree()) \n",
        "        out_degree_list=[x[1] for x in node_outdegree_tuple_list]\n",
        "        trace_outdegree = math.sqrt(sum(map(lambda x:x*x,out_degree_list)))\n",
        "        out_degree.append(trace_outdegree)\n",
        "    return out_degree"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeI7DQh1AF_g"
      },
      "source": [
        "Calculate cyclicity as number of cyclic node/number of nodes for each trace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJXZ8btAg62G"
      },
      "source": [
        "def cyclicity(log):\n",
        "    import networkx as nx\n",
        "    from pandas import DataFrame\n",
        "    cyclicity_list = []\n",
        "    trace_list = list(log.groupby(\"case:concept:name\", as_index=False))\n",
        "    for i in range(len(trace_list)):\n",
        "    # retrieve cycles and set of nodes contained in the cycles\n",
        "          trace = DataFrame(trace_list[i][1])\n",
        "          graph = create_directed_graph(trace)\n",
        "          cycles = list(nx.simple_cycles(graph))\n",
        "          cycle_nodes = set()\n",
        "          for i in cycles:\n",
        "              if len(i) > 1:\n",
        "                  cycle_nodes = cycle_nodes.union(set(i))\n",
        "          cyclicity_list.append(len(list(cycle_nodes)) / number_of_nodes(trace))\n",
        "    return cyclicity_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pB4dpYCx37C"
      },
      "source": [
        "Calculate graph density for each trace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKNJcB9jx4Ku"
      },
      "source": [
        "def density(log):\n",
        "    import networkx as nx\n",
        "    from pandas import DataFrame\n",
        "    density_list = []\n",
        "    trace_list = list(log.groupby(\"case:concept:name\", as_index=False))\n",
        "    for i in range(len(trace_list)):\n",
        "        trace = DataFrame(trace_list[i][1])\n",
        "        graph = create_directed_graph(trace)\n",
        "        density_list.append(nx.density(graph))\n",
        "    return density_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqBjtHGQyDZO"
      },
      "source": [
        "Calculate syntactic node similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXk-A6C-yDlx"
      },
      "source": [
        "def syntactic_node_similarity(log):\n",
        "    from editdistance import distance\n",
        "    from pandas import DataFrame\n",
        "    syntactic_node_sim_list = []\n",
        "    trace_list = list(log.groupby(\"case:concept:name\", as_index=False))\n",
        "    for i in range(len(trace_list)):\n",
        "        trace = DataFrame(trace_list[i][1])\n",
        "        events = event_names(trace)\n",
        "        counter = 0\n",
        "        for i in events:\n",
        "            for j in events:\n",
        "                sim = 1 - (distance(i, j) / max(len(i), len(j)))\n",
        "                if sim >= 0.6:\n",
        "                    counter += 1\n",
        "        if len(events)==1:\n",
        "            print ('zero')\n",
        "        syntactic_node_sim_list.append((counter - len(events)) / (len(events) * (len(events-1)) - len(events)))\n",
        "    return syntactic_node_sim_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdPQjwrSyNme"
      },
      "source": [
        "Methods for creating an undirected graph and count cut vertices for each trace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27nO-yHPyNvQ"
      },
      "source": [
        "# Creates an adjacency matrix to then create an undirected graph\n",
        "def adjacency_matrix_undirected(log):\n",
        "    from pm4py.objects.dfg.retrieval.pandas import  get_dfg_graph\n",
        "    import numpy as np\n",
        "\n",
        "    event_ranking = ranking_dict(log)\n",
        "    connections = [list(i) for i in [*(get_dfg_graph(log,  measure=\"frequency\", sort_caseid_required=False, sort_timestamp_along_case_id=True, keep_once_per_case=False, window=1))]]\n",
        "\n",
        "    # Bring connections in rank format\n",
        "    for connection in range(len(connections)):\n",
        "        for elem in range(len(connections[connection])):\n",
        "            connections[connection][elem] = event_ranking[connections[connection][elem]]\n",
        "\n",
        "    # Create initial matrix\n",
        "    matrix_shape = len(event_names(log))\n",
        "    adjac_matrix = np.zeros(shape=(matrix_shape, matrix_shape))\n",
        "\n",
        "    # Fill matrix based on connections\n",
        "    for (i, j) in connections:\n",
        "        adjac_matrix[i][j] += 1\n",
        "        adjac_matrix[j][i] += 1\n",
        "    return np.asarray(adjac_matrix)\n",
        "\n",
        "# Create undirected graph\n",
        "def create_undirected_graph(log):\n",
        "    \n",
        "    import networkx as nx\n",
        "\n",
        "    DG = nx.Graph()\n",
        "    matrix = adjacency_matrix_undirected(log)\n",
        "    number_for_event = ranking_dict(log)\n",
        "    event_for_number = dict((y, x) for x, y in number_for_event.items())\n",
        "    for row in range(len(matrix)):\n",
        "        for event in range(len(matrix[row])):\n",
        "            if matrix[row][event] > 0:\n",
        "                DG.add_edge(event_for_number[row], event_for_number[event])\n",
        "                DG.add_edge(event_for_number[event], event_for_number[row])\n",
        "    return DG\n",
        "\n",
        "# Number of articulation points\n",
        "def number_of_articulation_points(log):\n",
        "    import networkx as nx\n",
        "    from pandas import DataFrame\n",
        "    nr_articulation_points_list = []\n",
        "    trace_list = list(log.groupby(\"case:concept:name\", as_index=False))\n",
        "    for i in range(len(trace_list)):\n",
        "        trace = DataFrame(trace_list[i][1])\n",
        "        graph = create_undirected_graph(trace)\n",
        "        nr_articulation_points_list.append(len(list(nx.articulation_points(graph))))\n",
        "    return nr_articulation_points_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZARpTrXytXT"
      },
      "source": [
        "Triple abstraction evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lADga2eVytmT"
      },
      "source": [
        "def triple_abstraction_evaluation(log):\n",
        "    from pm4py.objects.dfg.retrieval.pandas import get_freq_triples\n",
        "    triple_abs_list = []\n",
        "    trace_list = list(log.groupby(\"case:concept:name\", as_index=False))\n",
        "    for i in range(len(trace_list)):\n",
        "        trace = pd.DataFrame(trace_list[i][1])\n",
        "        triples = list(get_freq_triples(trace).keys())\n",
        "        target_triples = set()\n",
        "        for i in triples:\n",
        "            for j in triples:\n",
        "                if i is not j:\n",
        "                    if i[0] == j[0] and i[-1] == j[-1]:\n",
        "                        target_triples.add(i)\n",
        "                        target_triples.add(j)\n",
        "        triple_abs_list.append(len(target_triples) / len(triples))\n",
        "    return triple_abs_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpzY5b8xAK_a"
      },
      "source": [
        "Create the dataset with all the features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEXeA5iFb5Ls"
      },
      "source": [
        "def create_feature_representation(log):\n",
        "    #take into account only control-flow\n",
        "    log = log[['case:concept:name', 'concept:name', 'time:timestamp']]\n",
        "    #Initialize the dataframe and append all the features\n",
        "    df = log['case:concept:name'].unique()\n",
        "    df = pd.DataFrame(data=df, columns=[\"case:concept:name\"])\n",
        "    df['event_count']= event_count(log)\n",
        "    df['event_count_unique'] = event_count_unique(log)\n",
        "    df['variant_code']= variant(log)\n",
        "    df['self_loop_per_trace_overview'] = self_loop_per_trace_overview(log)\n",
        "    df['repetition_per_trace_overview'] = repetition_per_trace_overview(log)\n",
        "    df['cat2num'] = cat2num(log)\n",
        "    df['network_connectivity'] = network_connectivity(log)\n",
        "    df['density'] = density(log)\n",
        "    df['syntactic_node_similarity'] = syntactic_node_similarity(log)\n",
        "    df['number_of_cut_vertices'] = number_of_cut_vertices(log)\n",
        "    df['triple_abstraction_evaluation'] = triple_abstraction_evaluation(log)\n",
        "    df['in_degree'] =in_degree(log)\n",
        "    df['out_degree'] = out_degree(log)\n",
        "    df['cyclicity'] = cyclicity(log)\n",
        "    return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzgBUJf7aCFR"
      },
      "source": [
        "\n",
        "\n",
        "# Feature Transformation (optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQkdgjJxaIzJ"
      },
      "source": [
        "#transform feature via PCA, n is number of components (transformed features)\n",
        "def transform_via_pca(log,n):\n",
        "    from sklearn.decomposition import PCA\n",
        "    log = PCA(n_components=n).fit(log)\n",
        "    return log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9dNqG3Cam2Y"
      },
      "source": [
        "#transform feature via LSA (truncated-SVD), n is number of components (transformed features)\n",
        "def transform_via_pca(log,n):\n",
        "    from sklearn.decomposition import TruncatedSVD\n",
        "    svd = TruncatedSVD(n_components=n)\n",
        "    #uncomment to print the explained variance by the components\n",
        "    #print(np.cumsum(svd.explained_variance_ratio_))\n",
        "    return svd.fit(log)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lq565kqdvFB"
      },
      "source": [
        "# import preprocessing\n",
        "from sklearn import preprocessing\n",
        "import numpy as np\n",
        "\n",
        "# Create correlation matrix\n",
        "def corr_matrix(df):\n",
        "    corr_matrix = df.corr().abs()\n",
        "    return corr_matrix\n",
        "\n",
        "# Select upper triangle of correlation matrix\n",
        "def upper(df):\n",
        "    upper = corr_matrix(df).where(np.triu(np.ones(corr_matrix(df).shape), k=1).astype(np.bool))\n",
        "    return upper\n",
        "\n",
        "# Find features with correlation greater than 0.9\n",
        "def remove_correlation(df):\n",
        "    to_drop = [column for column in upper(df).columns if any(upper[column] > 0.90)]\n",
        "    df = df.drop(to_drop, axis=1)\n",
        "    return df\n",
        "\n",
        "\n",
        "# normalize features to a (0,1) range\n",
        "def normalization(df):\n",
        "    from sklearn import preprocessing\n",
        "    scaler = preprocessing.MinMaxScaler()\n",
        "    # preprocess the features ItemsBought and ItemsReturned\n",
        "    df[[\"event_count\",\"event_count_unique\",\"variant_code\",\"self_loop_per_trace_overview\",\"repetition_per_trace_overview\",\"cat2num\",\"network_connectivity\",\"density\",\"syntactic_node_similarity\",\"number_of_cut_vertices\",\"triple_abstraction_evaluation\",\"in_degree\",\"out_degree\",\"cyclicity\"]] = scaler.fit_transform(df[[\"event_count\",\"event_count_unique\",\"variant_code\",\"self_loop_per_trace_overview\",\"repetition_per_trace_overview\",\"cat2num\",\"network_connectivity\",\"density\",\"syntactic_node_similarity\",\"number_of_cut_vertices\",\"triple_abstraction_evaluation\", \"in_degree\",\"out_degree\", \"cyclicity\"]])\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mv5o2FRdbau6"
      },
      "source": [
        "# Stratified Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XCZv2SxbeGw"
      },
      "source": [
        "import numpy as np\n",
        "# sample the log using stratified random sampling; size is the percentage of the log to be represented in the sample, e.g., 0.1 (10%)\n",
        "# output: a dataframe with columns: case:concept:name, concept:name, time:timestamp\n",
        "def sampling(log,size):\n",
        "    log_csv_variants = log.groupby('case:concept:name')['concept:name'].apply(tuple).reset_index(name = 'variants')\n",
        "\n",
        "    #define total sample size desired\n",
        "    N = round(size * log['case:concept:name'].nunique())\n",
        "\n",
        "    #perform stratified random sampling\n",
        "    log_csv_variants = log_csv_variants.groupby('variants', group_keys=False).apply(lambda x: x.sample(int(np.rint(N*len(x)/len(log_csv_variants))))).sample(frac=1).reset_index(drop=True)\n",
        "    sample = log_csv_variants.merge(log, on='case:concept:name').drop(columns={'variants'})\n",
        "    return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo7xpVAvg2dO"
      },
      "source": [
        "# Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alje74HOjb2w"
      },
      "source": [
        "Evaluation of several clustering techniques in terms of precision, replay-fitness and F1-score.\n",
        "Process models are extracted from each sublog using Inductive Miner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_pXROcAL3e9"
      },
      "source": [
        "# Get average of a list\n",
        "def Average(list):\n",
        "    return sum(list) / len(list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJ4IuXbUhHle"
      },
      "source": [
        "## K-means"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuUbHSUthLSk"
      },
      "source": [
        "def K_means_evaluation(sample,df):\n",
        "    from sklearn.cluster import KMeans\n",
        "    from pm4py.algo.discovery.inductive import algorithm as inductive_miner\n",
        "    from pm4py.objects.conversion.log import converter as log_converter\n",
        "    from pm4py.algo.evaluation.replay_fitness import algorithm as replay_fitness_evaluator\n",
        "    from pm4py.algo.evaluation.precision import algorithm as precision_evaluator\n",
        "    from pm4py.algo.discovery.inductive.algorithm import Variants\n",
        "\n",
        "    #contain f1-score, precision and fitness for each parameter setting, eg. k=1,2,3 ... 10\n",
        "    #k=1 is the baseline, 1 cluster i.e., whole log/sample\n",
        "    f1_final_list=[]\n",
        "    precision_final_list = []\n",
        "    fitness_final_list = []\n",
        "\n",
        "    for i in range(1,11):     \n",
        "        estimator = KMeans(n_clusters = i)\n",
        "        estimator.fit(df[[\"event_count\",\"event_count_unique\",\"variant_code\",\"self_loop_per_trace_overview\",\"repetition_per_trace_overview\",\"cat2num\",\"network_connectivity\",\"density\",\"syntactic_node_similarity\",\"number_of_cut_vertices\",\"triple_abstraction_evaluation\", \"in_degree\", \"out_degree\", \"cyclicity\"]])\n",
        "        df_with_cluster = df.assign(cluster=estimator.labels_)\n",
        "\n",
        "        #contain f1-score,precision and fitness for each sublog\n",
        "        score = []\n",
        "        precisionlist = []\n",
        "        fitnesslist = []\n",
        "        for j in range (0,i):\n",
        "            df1 = df_with_cluster[df_with_cluster['cluster'] == j]\n",
        "            df1 = pd.merge(sample,df1,how=\"inner\",on=\"case:concept:name\",left_index=False,right_index=False,indicator=False,copy=True,)\n",
        "            event_log = log_converter.apply(df1)\n",
        "            #convert log to Petri net\n",
        "            net, im, fm = inductive_miner.apply(event_log,variant=Variants.IMf) # variant=Variants.IMd\n",
        "            #calculate precision for each sublog\n",
        "            prec = precision_evaluator.apply(event_log, net, im, fm, variant=precision_evaluator.Variants.ALIGN_ETCONFORMANCE) #.ETCONFORMANCE_TOKEN\n",
        "            #calculate replay-fitness for each sublog\n",
        "            fitness = replay_fitness_evaluator.apply(event_log, net, im, fm, variant=replay_fitness_evaluator.Variants.ALIGNMENT_BASED) #.TOKEN_BASED\n",
        "\n",
        "            #use this in case inductive miner variant is IMD\n",
        "            #f_score = (2*prec*fitness.get('log_fitness'))/(prec+fitness.get('log_fitness'))         \n",
        "            f_score = (2*prec*fitness.get('averageFitness'))/(prec+fitness.get('averageFitness'))\n",
        "\n",
        "            score.append(f_score)\n",
        "            precisionlist.append(prec)\n",
        "            fitnesslist.append(fitness.get('averageFitness'))\n",
        " \n",
        "        f1_final_list.append(Average(score))\n",
        "        precision_final_list.append(Average(precisionlist))\n",
        "        fitness_final_list.append(Average(fitnesslist))\n",
        "\n",
        "    return f1_final_list, precision_final_list, fitness_final_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pySSK9CguBmB"
      },
      "source": [
        "## Hierarchical Agglomerative Clustering\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-iYreI_uFFm"
      },
      "source": [
        "from pm4py.algo.discovery.inductive import algorithm as inductive_miner\n",
        "from pm4py.algo.discovery.inductive.algorithm import Variants\n",
        "from pm4py.algo.evaluation.replay_fitness import algorithm as replay_fitness_evaluator\n",
        "from pm4py.algo.evaluation.precision import algorithm as precision_evaluator\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "\n",
        "def HAC_evaluation(sample, df):\n",
        "\n",
        "    f1_final_list=[]\n",
        "    precision_final_list = []\n",
        "    fitness_final_list = []\n",
        "\n",
        "    # create the clustering\n",
        "    Z = linkage(df[[\"event_count\",\"event_count_unique\",\"variant_code\",\"self_loop_per_trace_overview\",\"repetition_per_trace_overview\",\"cat2num\",\"network_connectivity\",\"density\",\"syntactic_node_similarity\",\"number_of_cut_vertices\",\"triple_abstraction_evaluation\", \"in_degree\", \"out_degree\", \"cyclicity\"]], 'median')\n",
        "\n",
        "    # plot the dendrogram\n",
        "    #dendrogram(Z, labels=df['case:concept:name'].values)\n",
        "\n",
        "    # import fcluster to add clusterIDs\n",
        "    from scipy.cluster.hierarchy import fcluster\n",
        "    dendrogram(Z, truncate_mode='lastp', p=20)\n",
        "    for t in range(2,11):\n",
        "\n",
        "        score = []\n",
        "        precisionlist = []\n",
        "        fitnesslist = []\n",
        "\n",
        "        clusters = fcluster(Z, t=t, criterion='maxclust')\n",
        "        df_with_cluster = df.copy()\n",
        "        df_with_cluster['cluster'] = clusters\n",
        "        score=[]\n",
        "        for j in range (0,t):\n",
        "            df1 = df_with_cluster[df_with_cluster['cluster'] == j]\n",
        "            df1 = pd.merge(sample,df1,how=\"inner\",on=\"case:concept:name\",left_index=False,right_index=False,indicator=False,copy=True,)  \n",
        "            event_log = log_converter.apply(df1)\n",
        "            net, im, fm = inductive_miner.apply(event_log, variant=Variants.IMf)\n",
        "\n",
        "            prec = precision_evaluator.apply(event_log, net, im, fm, variant=precision_evaluator.Variants.ALIGN_ETCONFORMANCE)\n",
        "           \n",
        "            fitness = replay_fitness_evaluator.apply(event_log, net, im, fm, variant=replay_fitness_evaluator.Variants.ALIGNMENT_BASED)\n",
        "            \n",
        "            f_score = (2*prec*fitness.get('averageFitness'))/(prec+fitness.get('averageFitness'))\n",
        "\n",
        "            score.append(f_score)\n",
        "            precisionlist.append(prec)\n",
        "            fitnesslist.append(fitness.get('averageFitness'))          \n",
        "\n",
        "        f1_final_list.append(Average(score))\n",
        "        precision_final_list.append(Average(precisionlist))\n",
        "        fitness_final_list.append(Average(fitnesslist))\n",
        "\n",
        "    return f1_final_list, precision_final_list, fitness_final_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAyRu2UbxTFA"
      },
      "source": [
        "## Spectral Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6qkP1lvxXhA"
      },
      "source": [
        "from sklearn.cluster import SpectralClustering\n",
        "from pm4py.algo.discovery.inductive import algorithm as inductive_miner\n",
        "from pm4py.algo.discovery.inductive.algorithm import Variants\n",
        "from pm4py.algo.evaluation.replay_fitness import algorithm as replay_fitness_evaluator\n",
        "from pm4py.algo.evaluation.precision import algorithm as precision_evaluator\n",
        "import pandas as pd\n",
        "\n",
        "def SC_evaluation(sample, df):\n",
        "\n",
        "    f1_final_list=[]\n",
        "    precision_final_list = []\n",
        "    fitness_final_list = []\n",
        "\n",
        "    for i in range (2,11):\n",
        "        estimator = SpectralClustering(n_clusters=i, affinity='rbf').fit(df[['event_count','event_count_unique', 'variant_code','self_loop_per_trace_overview', 'repetition_per_trace_overview','cat2num', 'network_connectivity','density','syntactic_node_similarity','number_of_cut_vertices', 'triple_abstraction_evaluation','in_degree', 'out_degree', 'cyclicity']])\n",
        "        df_with_cluster = df.assign(cluster=estimator.labels_)\n",
        "        score = []\n",
        "        for j in range (0,len(set(list(estimator.labels_)))):\n",
        "            df1 = df_with_cluster[df_with_cluster['cluster'] == j]\n",
        "            df1 = pd.merge(log,df1,how=\"inner\",on=\"case:concept:name\",left_index=False,right_index=False,indicator=False,copy=True,)\n",
        "            event_log = log_converter.apply(df1)\n",
        "            net, im, fm = inductive_miner.apply(event_log, variant=Variants.IMf)\n",
        "            \n",
        "            prec = precision_evaluator.apply(event_log, net, im, fm, variant=precision_evaluator.Variants.ALIGN_ETCONFORMANCE)\n",
        "\n",
        "            fitness = replay_fitness_evaluator.apply(event_log, net, im, fm, variant=replay_fitness_evaluator.Variants.ALIGNMENT_BASED)\n",
        "            \n",
        "            f_score = (2*prec*fitness.get('averageFitness'))/(prec+fitness.get('averageFitness'))\n",
        "            \n",
        "            score.append(f_score)\n",
        "            precisionlist.append(prec)\n",
        "            fitnesslist.append(fitness.get('averageFitness'))\n",
        "\n",
        "        f1_final_list.append(Average(score))\n",
        "        precision_final_list.append(Average(precisionlist))\n",
        "        fitness_final_list.append(Average(fitnesslist))\n",
        "\n",
        "    return f1_final_list, precision_final_list, fitness_final_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDTTqdiTyZRK"
      },
      "source": [
        "## DBSCAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBJsn60qygtc"
      },
      "source": [
        "#select value for eps\n",
        "import numpy as np\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "#plot distances to k-nearest neighbor, df-feature representation, k-parameter\n",
        "def plot_distances(df,k):\n",
        "    X = df.copy()\n",
        "    X.drop(columns='case:concept:name', inplace=True)\n",
        "    neigh = NearestNeighbors(n_neighbors=k)\n",
        "    nbrs = neigh.fit(X)\n",
        "    distances, indices = nbrs.kneighbors(X)\n",
        "    distances = np.sort(distances, axis=0)\n",
        "    distances = distances[:,1]\n",
        "    plt.plot(distances)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6utXkahz738"
      },
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from pm4py.algo.discovery.inductive import algorithm as inductive_miner\n",
        "from pm4py.algo.discovery.inductive.algorithm import Variants\n",
        "from pm4py.algo.evaluation.replay_fitness import algorithm as replay_fitness_evaluator\n",
        "from pm4py.algo.evaluation.precision import algorithm as precision_evaluator\n",
        "\n",
        "def DBSCAN_evaluation(sample,df):\n",
        "\n",
        "    f1_final_list=[]\n",
        "    precision_final_list = []\n",
        "    fitness_final_list = []\n",
        "    n_clusters = []\n",
        "\n",
        "    for eps in range(10,14,1):\n",
        "        for min_samples in range(22,88,22):\n",
        "            estimator = DBSCAN(eps=eps/10.0, min_samples=min_samples).fit(df[[\"event_count\",\"event_count_unique\",\"variant_code\",\"self_loop_per_trace_overview\",\"repetition_per_trace_overview\",\"cat2num\",\"network_connectivity\",\"density\",\"syntactic_node_similarity\",\"number_of_cut_vertices\",\"triple_abstraction_evaluation\",\"in_degree\", \"out_degree\", \"cyclicity\"]])\n",
        "            df_with_cluster = df.assign(cluster=estimator.labels_)\n",
        "            score = []\n",
        "            for j in range (0,len(set(list(estimator.labels_)))):\n",
        "                df1 = df_with_cluster[df_with_cluster['cluster'] == j]\n",
        "                df1 = pd.merge(sample,df1,how=\"inner\",on=\"case:concept:name\",left_index=False,right_index=False,indicator=False,copy=True,)\n",
        "                event_log = log_converter.apply(df1)\n",
        "                net, im, fm = inductive_miner.apply(event_log, variant=Variants.IMf)\n",
        "                prec = precision_evaluator.apply(event_log, net, im, fm, variant=precision_evaluator.Variants.ALIGN_ETCONFORMANCE)\n",
        "\n",
        "                fitness = replay_fitness_evaluator.apply(event_log, net, im, fm, variant=replay_fitness_evaluator.Variants.ALIGNMENT_BASED)\n",
        "\n",
        "                f_score = (2*prec*fitness.get('averageFitness'))/(prec+fitness.get('averageFitness'))\n",
        "\n",
        "                score.append(f_score)\n",
        "                precisionlist.append(prec)\n",
        "                fitnesslist.append(fitness.get('averageFitness'))\n",
        "\n",
        "            f1_final_list.append(Average(score))\n",
        "            precision_final_list.append(Average(precisionlist))\n",
        "            fitness_final_list.append(Average(fitnesslist))\n",
        "            n_clusters = n_clusters.append(len(set(list(estimator.labels_))))\n",
        "    return n_clusters, f1_final_list, precision_final_list, fitness_final_list               \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lbfNCCz2qap"
      },
      "source": [
        "## OPTICS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THCX6sEd2rBI"
      },
      "source": [
        "from sklearn.cluster import OPTICS\n",
        "from pm4py.algo.discovery.inductive import algorithm as inductive_miner\n",
        "from pm4py.algo.discovery.inductive.algorithm import Variants\n",
        "from pm4py.algo.evaluation.replay_fitness import algorithm as replay_fitness_evaluator\n",
        "from pm4py.algo.evaluation.precision import algorithm as precision_evaluator\n",
        "\n",
        "def OPTICS_evaluation(sample,df):\n",
        "\n",
        "    f1_final_list=[]\n",
        "    precision_final_list = []\n",
        "    fitness_final_list = []\n",
        "    n_clusters = []\n",
        "\n",
        "    for eps in range(10,14,1):\n",
        "        for min_samples in range(22,88,22):\n",
        "            estimator = OPTICS(eps=eps/10.0, min_samples=min_samples).fit(df[[\"event_count\",\"event_count_unique\",\"variant_code\",\"self_loop_per_trace_overview\",\"repetition_per_trace_overview\",\"cat2num\",\"network_connectivity\",\"density\",\"syntactic_node_similarity\",\"number_of_cut_vertices\",\"triple_abstraction_evaluation\",\"in_degree\", \"out_degree\", \"cyclicity\"]])\n",
        "            df_with_cluster = df.assign(cluster=estimator.labels_)\n",
        "            score = []\n",
        "            for j in range (0,len(set(list(estimator.labels_)))):\n",
        "                df1 = df_with_cluster[df_with_cluster['cluster'] == j]\n",
        "                df1 = pd.merge(log,df1,how=\"inner\",on=\"case:concept:name\",left_index=False,right_index=False,indicator=False,copy=True,)\n",
        "                event_log = log_converter.apply(df1)\n",
        "                net, im, fm = inductive_miner.apply(event_log, variant=Variants.IMf)\n",
        "                prec = precision_evaluator.apply(event_log, net, im, fm, variant=precision_evaluator.Variants.ALIGN_ETCONFORMANCE)\n",
        "\n",
        "                fitness = replay_fitness_evaluator.apply(event_log, net, im, fm, variant=replay_fitness_evaluator.Variants.ALIGNMENT_BASED)\n",
        "\n",
        "                f_score = (2*prec*fitness.get('averageFitness'))/(prec+fitness.get('averageFitness'))\n",
        "\n",
        "                score.append(f_score)\n",
        "                precisionlist.append(prec)\n",
        "                fitnesslist.append(fitness.get('averageFitness'))\n",
        "\n",
        "            f1_final_list.append(Average(score))\n",
        "            precision_final_list.append(Average(precisionlist))\n",
        "            fitness_final_list.append(Average(fitnesslist))\n",
        "            n_clusters = n_clusters.append(len(set(list(estimator.labels_))))\n",
        "    return n_clusters, f1_final_list, precision_final_list, fitness_final_list "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wlwje3nu3uLn"
      },
      "source": [
        "# Ranking Context Attributes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6FagQGwn2sR"
      },
      "source": [
        "#replace the missing values of a specific numeric attribute with average of the column,,, can also use .median()\n",
        "def replace_with_mean(log, attribute):\n",
        "    mean = log[attribute].mean()\n",
        "    return log.fillna({attribute:mean})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPtuwtXrC7gu"
      },
      "source": [
        "#sum up event-level attributes to convert to trace-level attribute, e.g. sum up cost for each event, to get the total cost for the trace\n",
        "def sum_event_level_attr(log, attribute):\n",
        "    return log.groupby('case:concept:name')[attribute].sum().reset_index()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbYJAZbF3y7-"
      },
      "source": [
        "#Apply best performing clustering setting (e.g. K-means with k=3) to the whole log and append the context attributes\n",
        "\n",
        "def context_representation(log, df):\n",
        "    from sklearn.cluster import KMeans\n",
        "    estimator = KMeans(n_clusters = 3)\n",
        "    estimator.fit(df[[\"event_count_unique\",\"variant_code\",\"self_loop_per_trace_overview\",\"repetition_per_trace_overview\",\"cat2num\",\"network_connectivity\",\"density\",\"syntactic_node_similarity\",\"number_of_cut_vertices\",\"triple_abstraction_evaluation\",\"in_degree\", \"out_degree\", \"cyclicity\"]])\n",
        "    df_with_cluster = df.assign(cluster=estimator.labels_)\n",
        "    log = log.groupby('case:concept:name').first().reset_index()\n",
        "    context = df_with_cluster[['case:concept:name','cluster']].merge(log.drop(columns=['concept:name']), on=\"case:concept:name\", how=\"left\")\n",
        "    return context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsGYKqfG44Rq"
      },
      "source": [
        "# convert attributes that are numerical to categorical (discretization)\n",
        "# k - number of bins, user-defined\n",
        "def discretize(attribute, k):\n",
        "    import pandas as pd\n",
        "    return pd.cut(column, k, labels=[\"low\", \"medium\", \"high\"])\n",
        "\n",
        "# filter out attributes with very low variance (one value)\n",
        "def low_variance_attr(context):\n",
        "    for column in context.columns:\n",
        "        if context[column].nunique() == 1:\n",
        "           contex.drop(columns=column, inplace=True, axis=1)\n",
        "    return context\n",
        "\n",
        "\n",
        "# filter out attributes with very high variance (IDs) \n",
        "# n is the user-defined threshold (equal to number of features)\n",
        "def high_variance_attr(context, n):\n",
        "    for column in context.columns:\n",
        "        if context[column].nunique() > n:\n",
        "           context.drop(columns=column, inplace=True, axis =1)\n",
        "    return context\n",
        "\n",
        "\n",
        "# filter out attributes which have missing values that cannot be inferred from the log\n",
        "# first the missing values which can be replaced are filled, then the rest is filtered out\n",
        "def filter_missing(cotext):\n",
        "    return context.dropna(axis = 1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2LKL0AlE2VO"
      },
      "source": [
        "# Get average of a list\n",
        "def Average(list):\n",
        "    return sum(list) / len(list)\n",
        "# Rank the context attributes\n",
        "# Output: sorted distionary of key:attribute name and value:score.\n",
        "def ranking(context):\n",
        "    import numpy as np\n",
        "    from sklearn.feature_extraction.text import TfidfTransformer\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    rank = {}\n",
        "    tfidf = TfidfTransformer()\n",
        "    context['helper']=1\n",
        "    context.head()\n",
        "    columns = list(context.drop(columns=['cluster', 'helper']).columns)\n",
        "    for column in columns:\n",
        "        df_pivot = context.pivot_table(index='cluster', columns=str(column), values='helper', fill_value=0, aggfunc=np.sum)\n",
        "        df_pivot.reset_index(inplace=True)\n",
        "        df_pivot.drop(columns = 'cluster', inplace = True)\n",
        "        tfidf.fit(df_pivot, y=None)\n",
        "        tfidf_matrix = tfidf.transform(df_pivot, copy=True)\n",
        "        disimm = (1-cosine_similarity(tfidf_matrix))\n",
        "        rank.update({column: Average(disimm[np.triu_indices(context['cluster'].nunique(), k = 1)])})\n",
        "    return (sorted(rank.items(), key=lambda item: item[1]))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgYxrF05FPmH"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Visualize Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IL5hjMNMjK2u"
      },
      "source": [
        "Create a DT using the ground truth context attribute as the label and visualize it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBDXjBVGFTcL"
      },
      "source": [
        "#decision tree\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn import tree\n",
        "import graphviz \n",
        "\n",
        "\n",
        "# attribute - the target attribute(label)\n",
        "# n - maximum depth of the tree to display\n",
        "def visualize_DT(context, attribute, n)\n",
        "    data = context.drop(columns={'cluster', attribute})\n",
        "    target = context[attribute]\n",
        "    target = target.astype(str)\n",
        "    target1=target.copy()\n",
        "    data = data.astype(str) \n",
        "    data1=data.copy()\n",
        "    enc = OrdinalEncoder()\n",
        "    data = enc.fit_transform(data)\n",
        "    data = pd.DataFrame(data=data)\n",
        "\n",
        "    estimator = DecisionTreeClassifier(max_depth=n)\n",
        "    #apply the DT classifier\n",
        "    estimator.fit(data, target)\n",
        "\n",
        "    dot_data = tree.export_graphviz(estimator, out_file=None, feature_names=data1.columns, filled=True, rounded=True, special_characters=True) \n",
        "    graph = graphviz.Source(dot_data) \n",
        "    display(graph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GLT-fcMrLkm"
      },
      "source": [
        "# BPMN models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbGIy2xZrPhd"
      },
      "source": [
        "from pm4py.objects.conversion.log import converter as log_converter\n",
        "from pm4py.objects.conversion.process_tree import converter\n",
        "from pm4py.visualization.bpmn import visualizer\n",
        "\n",
        "def bpmn_visualizer(log):\n",
        "    log = log_converter.apply(log)\n",
        "    tree = pm4py.discover_process_tree_inductive(log)\n",
        "    bpmn_graph = converter.apply(tree, variant=converter.Variants.TO_BPMN)\n",
        "    bpmn_model = visualizer.apply(bpmn_graph)\n",
        "    visualizer.view(bpmn_model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}